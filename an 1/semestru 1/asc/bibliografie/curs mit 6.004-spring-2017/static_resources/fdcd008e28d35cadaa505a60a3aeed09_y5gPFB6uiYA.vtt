WEBVTT

00:00:00.880 --> 00:00:06.690
In discussing out-of-order superscalar pipelined
CPUs we commented that the costs grow very

00:00:06.690 --> 00:00:11.470
quickly relative to the performance gains,
leading to the cost-performance curve shown

00:00:11.470 --> 00:00:12.820
here.

00:00:12.820 --> 00:00:17.109
If we move down the curve, we can arrive at
more efficient architectures that give, say,

00:00:17.109 --> 00:00:20.189
1/2 the performance at a 1/4 of the cost.

00:00:20.189 --> 00:00:24.980
When our applications involve independent
computations that can be performed in a parallel,

00:00:24.980 --> 00:00:29.859
it may be that we would be able to use two
cores to provide the same performance as the

00:00:29.859 --> 00:00:34.400
original expensive core, but a fraction of
the cost.

00:00:34.400 --> 00:00:39.590
If the available parallelism allows us to
use additional cores, we'll see a linear relationship

00:00:39.590 --> 00:00:43.740
between increased performance vs. increased
cost.

00:00:43.740 --> 00:00:48.290
The key, of course, is that desired computations
can be divided into multiple tasks that can

00:00:48.290 --> 00:00:53.070
run independently, with little or no need
for communication or coordination between

00:00:53.070 --> 00:00:55.240
the tasks.

00:00:55.240 --> 00:01:00.160
What is the optimal tradeoff between core
cost and the number of cores?

00:01:00.160 --> 00:01:04.760
If our computation is arbitrarily divisible
without incurring additional overhead,

00:01:04.760 --> 00:01:09.000
then we would continue to move down the curve
until we found the cost-performance point

00:01:09.000 --> 00:01:12.380
that gave us the desired performance at the
least cost.

00:01:12.380 --> 00:01:18.260
In reality, dividing the computation across
many cores does involve some overhead, e.g.,

00:01:18.260 --> 00:01:23.499
distributing the data and code, then collecting
and aggregating the results, so the optimal

00:01:23.499 --> 00:01:25.729
tradeoff is harder to find.

00:01:25.729 --> 00:01:32.158
Still, the idea of using a larger number of
smaller, more efficient cores seems attractive.

00:01:32.158 --> 00:01:36.770
Many applications have some computations that
can be performed in parallel, but also have

00:01:36.770 --> 00:01:40.390
computations that won't benefit from parallelism.

00:01:40.390 --> 00:01:44.530
To understand the speedup we might expect
from exploiting parallelism, it's useful to

00:01:44.530 --> 00:01:50.899
perform the calculation proposed by computer
scientist Gene Amdahl in 1967, now known as

00:01:50.899 --> 00:01:53.119
Amdahl's Law.

00:01:53.119 --> 00:01:57.429
Suppose we're considering an enhancement that
speeds up some fraction F of the task at hand

00:01:57.429 --> 00:02:01.899
by a factor of S.
As shown in the figure, the gray portion of

00:02:01.899 --> 00:02:07.299
the task now takes F/S of the time that it
used to require.

00:02:07.299 --> 00:02:14.060
Some simple arithmetic lets us calculate the
overall speedup we get from using the enhancement.

00:02:14.060 --> 00:02:18.920
One conclusion we can draw is that we'll benefit
the most from enhancements that affect a large

00:02:18.920 --> 00:02:25.579
portion of the required computations, i.e.,
we want to make F as large a possible.

00:02:25.579 --> 00:02:29.980
What's the best speedup we can hope for if
we have many cores that can be used to speed

00:02:29.980 --> 00:02:32.700
up the parallel part of the task?

00:02:32.700 --> 00:02:38.700
Here's the speedup formula based on F and
S, where in this case F is the parallel fraction

00:02:38.700 --> 00:02:40.579
of the task.

00:02:40.579 --> 00:02:44.610
If we assume that the parallel fraction of
the task can be speed up arbitrarily by using

00:02:44.610 --> 00:02:49.220
more and more cores, we see that the best
possible overall speed up is 1/(1-F).

00:02:49.220 --> 00:02:57.020
For example, you write a program that can
do 90% of its work in parallel, but the other

00:02:57.020 --> 00:03:00.230
10% must be done sequentially.

00:03:00.230 --> 00:03:05.320
The best overall speedup that can be achieved
is a factor of 10, no matter how many cores

00:03:05.320 --> 00:03:08.250
you have at your disposal.

00:03:08.250 --> 00:03:12.230
Turning the question around, suppose you have
a 1000-core machine which you hope to be able

00:03:12.230 --> 00:03:16.960
to use to achieve a speedup of 500 on your
target application.

00:03:16.960 --> 00:03:22.970
You would need to be able parallelize 99.8%
of the computation in order to reach your

00:03:22.970 --> 00:03:24.290
goal!

00:03:24.290 --> 00:03:32.170
Clearly multicore machines are most useful
when the target task has lots of natural parallelism.

00:03:32.170 --> 00:03:37.440
Using multiple independent cores to execute
a parallel task is called thread-level parallelism

00:03:37.440 --> 00:03:42.180
(TLP), where each core executes a separate
computation "thread".

00:03:42.180 --> 00:03:47.000
The threads are independent programs, so the
execution model is potentially more flexible

00:03:47.000 --> 00:03:52.120
than the lock-step execution provided by vector
machines.

00:03:52.120 --> 00:03:56.300
When there are a small number of threads,
you often see the cores sharing a common main

00:03:56.300 --> 00:04:01.030
memory, allowing the threads to communicate
and synchronize by sharing a common address

00:04:01.030 --> 00:04:02.030
space.

00:04:02.030 --> 00:04:05.230
We'll discuss this further in the next section.

00:04:05.230 --> 00:04:12.490
This is the approach used in current multicore
processors, which have between 2 and 12 cores.

00:04:12.490 --> 00:04:17.380
Shared memory becomes a real bottleneck when
there 10's or 100's of cores, since collectively

00:04:17.380 --> 00:04:20.490
they quickly overwhelm the available memory
bandwidth.

00:04:20.490 --> 00:04:26.250
In these architectures, threads communicate
using a communication network to pass messages

00:04:26.250 --> 00:04:28.210
back and forth.

00:04:28.210 --> 00:04:32.470
We discussed possible network topologies in
an earlier lecture.

00:04:32.470 --> 00:04:38.170
A cost-effective on-chip approach is to use
a nearest-neighbor mesh network, which supports

00:04:38.170 --> 00:04:43.790
many parallel point-to-point communications,
while still allowing multi-hop communication

00:04:43.790 --> 00:04:46.870
between any two cores.

00:04:46.870 --> 00:04:52.460
Message passing is also used in computing
clusters, where many ordinary CPUs collaborate

00:04:52.460 --> 00:04:54.560
on large tasks.

00:04:54.560 --> 00:04:58.590
There's a standardized message passing interface
(MPI) and

00:04:58.590 --> 00:05:04.750
specialized, very high throughput, low latency
message-passing communication networks (e.g.,

00:05:04.750 --> 00:05:09.310
Infiniband) that make it easy to build high-performance
computing clusters.

00:05:09.310 --> 00:05:13.970
In the next couple of sections we'll look
more closely at some of the issues involved

00:05:13.970 --> 00:05:16.650
in building shared-memory multicore processors.