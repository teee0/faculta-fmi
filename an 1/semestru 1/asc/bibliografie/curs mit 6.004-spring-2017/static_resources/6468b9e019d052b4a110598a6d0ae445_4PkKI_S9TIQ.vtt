WEBVTT

00:00:01.020 --> 00:00:06.319
Let's wrap up our discussion of system-level
interconnect by considering how best to connect

00:00:06.319 --> 00:00:13.670
N components that need to send messages to
one another, e.g., CPUs on a multicore chip.

00:00:13.670 --> 00:00:19.520
Today such chips have a handful of cores,
but soon they may have 100s or 1000s of cores.

00:00:19.520 --> 00:00:23.390
We'll build our communications network using
point-to-point links.

00:00:23.390 --> 00:00:28.890
In our analysis, each point-to-point link
is counted at a cost of 1 hardware unit.

00:00:28.890 --> 00:00:32.960
Sending a message across a link requires one
time unit.

00:00:32.960 --> 00:00:36.980
And we'll assume that different links can
operate in parallel, so more links will mean

00:00:36.980 --> 00:00:38.640
more message traffic.

00:00:38.640 --> 00:00:46.570
We'll do an asymptotic analysis of the throughput
(total messages per unit time), latency (worst-case

00:00:46.570 --> 00:00:49.899
time to deliver a single message), and hardware
cost.

00:00:49.899 --> 00:00:57.290
In other words, we'll make a rough estimate
how these quantities change as N grows.

00:00:57.290 --> 00:01:02.480
Note that in general the throughput and hardware
cost are proportional to the number of point-to-point

00:01:02.480 --> 00:01:03.700
links.

00:01:03.700 --> 00:01:08.240
Our baseline is the backplane bus discussed
earlier, where all the components share a

00:01:08.240 --> 00:01:10.749
single communication channel.

00:01:10.749 --> 00:01:16.600
With only a single channel, bus throughput
is 1 message per unit time and a message can

00:01:16.600 --> 00:01:20.200
travel between any two components in one time
unit.

00:01:20.200 --> 00:01:24.729
Since each component has to have an interface
to the shared channel, the total hardware

00:01:24.729 --> 00:01:27.170
cost is O(n).

00:01:27.170 --> 00:01:32.079
In a ring network each component sends its
messages to a single neighbor and the links

00:01:32.079 --> 00:01:35.529
are arranged so that its possible to reach
all components.

00:01:35.529 --> 00:01:42.450
There are N links in total, so the throughput
and cost are both O(n).

00:01:42.450 --> 00:01:47.959
The worst case latency is also O(n) since
a message might have to travel across N-1

00:01:47.959 --> 00:01:52.009
links to reach the neighbor that's immediately
upstream.

00:01:52.009 --> 00:01:57.229
Ring topologies are useful when message latency
isn't important or when most messages are

00:01:57.229 --> 00:02:03.479
to the component that's immediately downstream,
i.e., the components form a processing pipeline.

00:02:03.479 --> 00:02:08.280
The most general network topology is when
every component has a direct link to every

00:02:08.280 --> 00:02:10.000
other component.

00:02:10.000 --> 00:02:15.280
There are O(N**2) links so the throughput
and cost are both O(N**2).

00:02:15.280 --> 00:02:21.970
And the latency is 1 time unit since each
destination is directly accessible.

00:02:21.970 --> 00:02:28.200
Although expensive, complete graphs offer
very high throughput with very low latencies.

00:02:28.200 --> 00:02:34.050
A variant of the complete graph is the crossbar
switch where a particular row and column can

00:02:34.050 --> 00:02:38.490
be connected to form a link between particular
A and B components

00:02:38.490 --> 00:02:43.260
with the restriction that each row and each
column can only carry 1 message during each

00:02:43.260 --> 00:02:44.930
time unit.

00:02:44.930 --> 00:02:49.329
Assume that the first row and first column
connect to the same component, and so on,

00:02:49.329 --> 00:02:54.579
i.e., that the example crossbar switch is
being used to connect 4 components.

00:02:54.579 --> 00:03:00.430
Then there are O(n) messages delivered each
time unit, with a latency of 1.

00:03:00.430 --> 00:03:05.660
There are N**2 switches in the crossbar, so
the cost is O(N**2) even though there are

00:03:05.660 --> 00:03:08.470
only O(n) links.

00:03:08.470 --> 00:03:13.410
In mesh networks, components are connected
to some fixed number of neighboring components,

00:03:13.410 --> 00:03:16.160
in either 2 or 3 dimensions.

00:03:16.160 --> 00:03:21.530
Hence the total number of links is proportional
to the number of components, so both throughput

00:03:21.530 --> 00:03:24.079
and cost are O(n).

00:03:24.079 --> 00:03:28.670
The worst-case latencies for mesh networks
are proportional to length of the sides, so

00:03:28.670 --> 00:03:35.360
the latency is O(sqrt n) for 2D meshes and
O(cube root n) for 3D meshes.

00:03:35.360 --> 00:03:41.140
The orderly layout, constant per-node hardware
costs, and modest worst-case latency make

00:03:41.140 --> 00:03:47.210
2D 4-neighbor meshes a popular choice for
the current generation of experimental multi-core

00:03:47.210 --> 00:03:48.930
processors.

00:03:48.930 --> 00:03:54.540
Hypercube and tree networks offer logarithmic
latencies, which for large N may be faster

00:03:54.540 --> 00:03:56.680
than mesh networks.

00:03:56.680 --> 00:04:02.350
The original CM-1 Connection Machine designed
in the 80's used a hypercube network to connect

00:04:02.350 --> 00:04:10.300
up to 65,536 very simple processors, each
connected to 16 neighbors.

00:04:10.300 --> 00:04:15.600
Later generations incorporated smaller numbers
of more sophisticated processors, still connected

00:04:15.600 --> 00:04:18.149
by a hypercube network.

00:04:18.149 --> 00:04:23.910
In the early 90's the last generation of Connection
Machines used a tree network, with the clever

00:04:23.910 --> 00:04:29.509
innovation that the links towards the root
of the tree had a higher message capacity.

00:04:29.509 --> 00:04:34.970
Here's a summary of the theoretical latencies
we calculated for the various topologies.

00:04:34.970 --> 00:04:39.360
As a reality check, it's important to realize
that the lower bound on the worst-case distance

00:04:39.360 --> 00:04:43.460
between components in our 3-dimensional world
is O(cube root of N).

00:04:43.460 --> 00:04:49.470
In the case of a 2D layout, the worst-case
distance is O(sqrt N).

00:04:49.470 --> 00:04:54.210
Since we know that the time to transmit a
message is proportional to the distance traveled,

00:04:54.210 --> 00:05:00.229
we should modify our latency calculations
to reflect this physical constraint.

00:05:00.229 --> 00:05:06.009
Note that the bus and crossbar involve N connections
to a single link, so here the lower-bound

00:05:06.009 --> 00:05:11.699
on the latency needs to reflect the capacitive
load added by each connection.

00:05:11.699 --> 00:05:13.130
The winner?

00:05:13.130 --> 00:05:18.279
Mesh networks avoid the need for longer wires
as the number of connected components grows

00:05:18.279 --> 00:05:23.569
and appear to be an attractive alternative
for high-capacity communication networks connecting

00:05:23.569 --> 00:05:26.400
1000's of processors.

00:05:26.400 --> 00:05:30.259
Summarizing our discussion:
point-to-point links are in common use today

00:05:30.259 --> 00:05:35.720
for system-level interconnect, and as a result
our systems are faster, more reliable, more

00:05:35.720 --> 00:05:39.189
energy-efficient and smaller than ever before.

00:05:39.189 --> 00:05:44.740
Multi-signal parallel buses are still used
for very-high-bandwidth connections to memories,

00:05:44.740 --> 00:05:49.919
with a lot of very careful engineering to
avoid the electrical problems observed in

00:05:49.919 --> 00:05:53.819
earlier bus implementations.

00:05:53.819 --> 00:05:58.479
Wireless connections are in common use to
connect mobile devices to nearby components

00:05:58.479 --> 00:06:03.490
and there has been interesting work on how
to allow mobile devices to discover what peripherals

00:06:03.490 --> 00:06:07.819
are nearby and enable them to connect automatically.

00:06:07.819 --> 00:06:12.439
The upcoming generation of multi-core chips
will have 10's to 100's of processing cores.

00:06:12.439 --> 00:06:17.729
There is a lot ongoing research to determine
which communication topology would offer the

00:06:17.729 --> 00:06:23.169
best combination of high communication bandwidth
and low latency.

00:06:23.169 --> 00:06:27.459
The next ten years will be an interesting
time for on-chip network engineers!