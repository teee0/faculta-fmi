WEBVTT

00:00:01.300 --> 00:00:07.200
The simplest cache hardware consists of an
SRAM with a few additional pieces of logic.

00:00:07.200 --> 00:00:12.130
The cache hardware is designed so that each
memory location in the CPU's address space

00:00:12.130 --> 00:00:18.249
maps to a particular cache line, hence the
name "direct-mapped (DM) cache".

00:00:18.249 --> 00:00:23.079
There are, of course, many more memory locations
then there are cache lines, so many addresses

00:00:23.079 --> 00:00:27.650
are mapped to the same cache line and the
cache will only be able to hold the data for

00:00:27.650 --> 00:00:31.169
one of those addresses at a time.

00:00:31.169 --> 00:00:34.250
The operation of a DM cache is straightforward.

00:00:34.250 --> 00:00:39.240
We'll use part of the incoming address as
an index to select a single cache line to

00:00:39.240 --> 00:00:40.899
be searched.

00:00:40.899 --> 00:00:46.129
The "search" consists of comparing the rest
of the incoming address with the address tag

00:00:46.129 --> 00:00:48.600
of the selected cache line.

00:00:48.600 --> 00:00:53.839
If the tag matches the address, there's a
cache hit and we can immediately use the data

00:00:53.839 --> 00:00:58.160
in the cache to satisfy the request.

00:00:58.160 --> 00:01:03.570
In this design, we've included an additional
"valid bit" which is 1 when the tag and data

00:01:03.570 --> 00:01:06.590
fields hold valid information.

00:01:06.590 --> 00:01:12.110
The valid bit for each cache line is initialized
to 0 when the cache is powered on, indicating

00:01:12.110 --> 00:01:15.830
that all cache lines are empty.

00:01:15.830 --> 00:01:21.070
As data is brought into the cache, the valid
bit is set to 1 when the cache line's tag

00:01:21.070 --> 00:01:24.110
and data fields are filled.

00:01:24.110 --> 00:01:28.460
The CPU can request that the valid bit be
cleared for a particular cache line - this

00:01:28.460 --> 00:01:30.550
is called "flushing the cache".

00:01:30.550 --> 00:01:35.840
If, for example, the CPU initiates a read
from disk, the disk hardware will read its

00:01:35.840 --> 00:01:42.570
data into a block of main memory, so any cached
values for that block will out-of-date.

00:01:42.570 --> 00:01:48.750
So the CPU will flush those locations from
the cache by marking any matching cache lines

00:01:48.750 --> 00:01:50.610
as invalid.

00:01:50.610 --> 00:01:56.580
Let's see how this works using a small DM
cache with 8 lines where each cache line contains

00:01:56.580 --> 00:02:00.200
a single word (4 bytes) of data.

00:02:00.200 --> 00:02:05.390
Here's a CPU request for the location at byte
address 0xE8.

00:02:05.390 --> 00:02:10.228
Since there 4 bytes of data in each cache
line, the bottom 2 address bits indicate the

00:02:10.228 --> 00:02:14.390
appropriate byte offset into the cached word.

00:02:14.390 --> 00:02:20.160
Since the cache deals only with word accesses,
the byte offset bits aren't used.

00:02:20.160 --> 00:02:27.060
Next, we'll need to use 3 address bits to
select which of the 8 cache lines to search.

00:02:27.060 --> 00:02:31.700
We choose these cache index bits from the
low-order bits of the address.

00:02:31.700 --> 00:02:32.750
Why?

00:02:32.750 --> 00:02:36.100
Well, it's because of locality.

00:02:36.100 --> 00:02:40.430
The principle of locality tells us that it's
likely that the CPU will be requesting nearby

00:02:40.430 --> 00:02:45.980
addresses and for the cache to perform well,
we'd like to arrange for nearby locations

00:02:45.980 --> 00:02:50.400
to be able to be held in the cache at the
same time.

00:02:50.400 --> 00:02:56.250
This means that nearby locations will have
to be mapped to different cache lines.

00:02:56.250 --> 00:03:01.280
The addresses of nearby locations differ in
their low-order address bits, so we'll use

00:03:01.280 --> 00:03:06.840
those bits as the cache index bits - that
way nearby locations will map to different

00:03:06.840 --> 00:03:09.370
cache lines.

00:03:09.370 --> 00:03:15.630
The data, tag and valid bits selected by the
cache line index are read from the SRAM.

00:03:15.630 --> 00:03:20.490
To complete the search, we check the remaining
address against the tag field of the cache.

00:03:20.490 --> 00:03:25.940
If they're equal and the valid bit is 1, we
have a cache hit, and the data field can be

00:03:25.940 --> 00:03:29.740
used to satisfy the request.

00:03:29.740 --> 00:03:35.260
How come the tag field isn't 32 bits, since
we have a 32-bit address?

00:03:35.260 --> 00:03:39.620
We could have done that, but since all values
stored in cache line 2 will have the same

00:03:39.620 --> 00:03:46.860
index bits (0b010), we saved a few bits of
SRAM and chose not save those bits in the

00:03:46.860 --> 00:03:47.860
tag.

00:03:47.860 --> 00:03:52.620
In other words, there's no point in using
SRAM to save bits we can generate from the

00:03:52.620 --> 00:03:55.180
incoming address.

00:03:55.180 --> 00:04:02.380
So the cache hardware in this example is an
8-location by 60 bit SRAM plus a 27-bit comparator

00:04:02.380 --> 00:04:04.730
and a single AND gate.

00:04:04.730 --> 00:04:10.220
The cache access time is the access time of
the SRAM plus the propagation delays of the

00:04:10.220 --> 00:04:12.459
comparator and AND gate.

00:04:12.459 --> 00:04:16.380
About as simple and fast as we could hope
for.

00:04:16.380 --> 00:04:22.120
The downside of the simplicity is that for
each CPU request, we're only looking in a

00:04:22.120 --> 00:04:27.599
single cache location to see if the cache
holds the desired data.

00:04:27.599 --> 00:04:29.819
Not much of "search" is it?

00:04:29.819 --> 00:04:33.770
But the mapping of addresses to cache lines
helps us out here.

00:04:33.770 --> 00:04:38.180
Using the low-order address bit as the cache
index, we've arranged for nearby locations

00:04:38.180 --> 00:04:40.449
to be mapped to different cache lines.

00:04:40.449 --> 00:04:45.240
So, for example, if the CPU were executing
an 8-instruction loop, all 8 instructions

00:04:45.240 --> 00:04:47.689
can be held in the cache at the same time.

00:04:47.689 --> 00:04:52.308
A more complicated search mechanism couldn't
improve on that.

00:04:52.308 --> 00:04:57.479
The bottom line: this extremely simple "search"
is sufficient to get good cache hit ratios

00:04:57.479 --> 00:05:00.349
for the cases we care about.

00:05:00.349 --> 00:05:06.310
Let's try a few more examples, in this case
using a DM cache with 64 lines.

00:05:06.310 --> 00:05:11.550
Suppose the cache gets a read request for
location 0x400C.

00:05:11.550 --> 00:05:16.740
To see how the request is processed, we first
write the address in binary so we can easily

00:05:16.740 --> 00:05:21.310
divide it into the offset, index and tag fields.

00:05:21.310 --> 00:05:26.749
For this address the offset bits have the
value 0, the cache line index bits have the

00:05:26.749 --> 00:05:32.009
value 3, and the tag bits have the value 0x40.

00:05:32.009 --> 00:05:37.520
So the tag field of cache line 3 is compared
with the tag field of the address.

00:05:37.520 --> 00:05:42.240
Since there's a match, we have a cache hit
and the value in the data field of cache line

00:05:42.240 --> 00:05:46.029
can be used to satisfy the request.

00:05:46.029 --> 00:05:50.900
Would an access to location 0x4008 be a cache
hit?

00:05:50.900 --> 00:05:55.379
This address is similar to that in our first
example, except the cache line index is now

00:05:55.379 --> 00:05:58.449
2 instead of 3.

00:05:58.449 --> 00:06:04.330
Looking in cache line 2, we that its tag field
(0x58) doesn't match the tag field in the

00:06:04.330 --> 00:06:08.159
address (0x40), so this access would be a
cache miss.

00:06:08.159 --> 00:06:14.930
What are the addresses of the words held by
cache lines 0, 1, and 2, all of which have

00:06:14.930 --> 00:06:16.850
the same tag field?

00:06:16.850 --> 00:06:22.110
Well, we can run the address matching process
backwards!

00:06:22.110 --> 00:06:26.219
For an address to match these three cache
lines it would have look like the binary shown

00:06:26.219 --> 00:06:30.370
here, where we've used the information in
the cache tag field to fill in the high-order

00:06:30.370 --> 00:06:35.680
address bits and low-order address bits will
come from the index value.

00:06:35.680 --> 00:06:41.150
If we fill in the indices 0, 1, and 2, then
convert the resulting binary to hex we get

00:06:41.150 --> 00:06:48.650
0x5800, 0x5804, and 0x5808 as the addresses
for the data held in cache lines 0, 1, and

00:06:48.650 --> 00:06:51.150
2.

00:06:51.150 --> 00:06:56.089
Note that the complete address of the cached
locations is formed by combining the tag field

00:06:56.089 --> 00:06:59.899
of the cache line with the index of the cache
line.

00:06:59.899 --> 00:07:04.050
We of course need to be able to recover the
complete address from the information held

00:07:04.050 --> 00:07:09.179
in the cache so it can be correctly compared
against address requests from the CPU.