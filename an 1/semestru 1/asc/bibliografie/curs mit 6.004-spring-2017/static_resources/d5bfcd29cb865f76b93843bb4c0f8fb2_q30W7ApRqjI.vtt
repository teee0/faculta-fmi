WEBVTT

00:00:00.930 --> 00:00:05.180
A fully-associative (FA) cache has a tag comparator
for each cache line.

00:00:05.180 --> 00:00:09.980
So the tag field of *every* cache line in
a FA cache is compared with the tag field

00:00:09.980 --> 00:00:12.740
of the incoming address.

00:00:12.740 --> 00:00:18.290
Since all cache lines are searched, a particular
memory location can be held in any cache line,

00:00:18.290 --> 00:00:24.660
which eliminates the problems of address conflicts
causing conflict misses.

00:00:24.660 --> 00:00:31.119
The cache shown here can hold 4 different
4-word blocks, regardless of their address.

00:00:31.119 --> 00:00:35.850
The example from the end of the previous segment
required a cache that could hold two 3-word

00:00:35.850 --> 00:00:40.640
blocks, one for the instructions in the loop,
and one for the data words.

00:00:40.640 --> 00:00:47.510
This FA cache would use two of its cache lines
to perform that task and achieve a 100% hit

00:00:47.510 --> 00:00:53.110
ratio regardless of the addresses of the instruction
and data blocks.

00:00:53.110 --> 00:00:59.840
FA caches are very flexible and have high
hit ratios for most applications.

00:00:59.840 --> 00:01:03.020
Their only downside is cost.

00:01:03.020 --> 00:01:07.710
The inclusion of a tag comparator for each
cache line to implement the parallel search

00:01:07.710 --> 00:01:13.500
for a tag match adds substantially the amount
of circuitry required when there are many

00:01:13.500 --> 00:01:15.680
cache lines.

00:01:15.680 --> 00:01:21.560
Even the use of hybrid storage/comparison
circuitry, called a content-addressable memory,

00:01:21.560 --> 00:01:27.160
doesn't make a big dent in the overall cost
of a FA cache.

00:01:27.160 --> 00:01:31.310
DM caches searched only a single cache line.

00:01:31.310 --> 00:01:34.770
FA caches search all cache lines.

00:01:34.770 --> 00:01:39.229
Is there a happy middle ground where some
small number of cache lines are searched in

00:01:39.229 --> 00:01:40.229
parallel?

00:01:40.229 --> 00:01:41.229
Yes!

00:01:41.229 --> 00:01:46.799
If you look closely at the diagram of the
FA cache shown here, you'll see it looks like

00:01:46.799 --> 00:01:52.189
four 1-line DM caches operating in parallel.

00:01:52.189 --> 00:01:58.810
What would happen if we designed a cache with
four multi-line DM caches operating in parallel?

00:01:58.810 --> 00:02:03.590
The result would be what we call an 4-way
set-associative (SA) cache.

00:02:03.590 --> 00:02:10.288
An N-way SA cache is really just N DM caches
(let's call them sub-caches) operating in

00:02:10.288 --> 00:02:11.939
parallel.

00:02:11.939 --> 00:02:16.889
Each of the N sub-caches compares the tag
field of the incoming address with the tag

00:02:16.889 --> 00:02:22.629
field of the cache line selected by the index
bits of the incoming address.

00:02:22.629 --> 00:02:28.769
The N cache lines searched on a particular
request form a search "set" and the desired

00:02:28.769 --> 00:02:33.129
location might be held in any member of the
set.

00:02:33.129 --> 00:02:39.950
The 4-way SA cache shown here has 8 cache
lines in each sub-cache, so each set contains

00:02:39.950 --> 00:02:45.849
4 cache lines (one from each sub-cache) and
there are a total of 8 sets (one for each

00:02:45.849 --> 00:02:50.040
line of the sub-caches).

00:02:50.040 --> 00:02:55.569
An N-way SA cache can accommodate up to N
blocks whose addresses map to the same cache

00:02:55.569 --> 00:02:57.709
index.

00:02:57.709 --> 00:03:03.089
So access to up to N blocks with conflicting
addresses can still be accommodated in this

00:03:03.089 --> 00:03:05.760
cache without misses.

00:03:05.760 --> 00:03:11.370
This a big improvement over a DM cache where
an address conflict will cause the current

00:03:11.370 --> 00:03:16.930
resident of a cache line to be evicted in
favor of the new request.

00:03:16.930 --> 00:03:22.639
And an N-way SA cache can have a very large
number of cache lines but still only have

00:03:22.639 --> 00:03:26.540
to pay the cost of N tag comparators.

00:03:26.540 --> 00:03:31.809
This is a big improvement over a FA cache
where a large number of cache lines would

00:03:31.809 --> 00:03:35.349
require a large number of comparators.

00:03:35.349 --> 00:03:43.019
So N-way SA caches are a good compromise between
a conflict-prone DM cache and the flexible

00:03:43.019 --> 00:03:47.089
but very expensive FA cache.

00:03:47.089 --> 00:03:53.159
Here's a slightly more detailed diagram, in
this case of a 3-way 8-set cache.

00:03:53.159 --> 00:03:57.299
Note that there's no constraint that the number
of ways be a power of two since we aren't

00:03:57.299 --> 00:04:01.249
using any address bits to select a particular
way.

00:04:01.249 --> 00:04:07.569
This means the cache designer can fine tune
the cache capacity to fit her space budget.

00:04:07.569 --> 00:04:12.309
Just to review the terminology: the N cache
lines that will be searched for a particular

00:04:12.309 --> 00:04:14.459
cache index are called a set.

00:04:14.459 --> 00:04:19.829
And each of N sub-caches is called a way.

00:04:19.829 --> 00:04:24.840
The hit logic in each "way" operates in parallel
with the logic in other ways.

00:04:24.840 --> 00:04:30.160
Is it possible for a particular address to
be matched by more than one way?

00:04:30.160 --> 00:04:36.300
That possibility isn't ruled out by the hardware,
but the SA cache is managed so that doesn't

00:04:36.300 --> 00:04:37.530
happen.

00:04:37.530 --> 00:04:42.590
Assuming we write the data fetched from DRAM
during a cache miss into a single sub-cache

00:04:42.590 --> 00:04:46.439
- we'll talk about how to choose that way
in a minute -

00:04:46.439 --> 00:04:52.710
there's no possibility that more than one
sub-cache will ever match an incoming address.

00:04:52.710 --> 00:04:54.680
How many ways to do we need?

00:04:54.680 --> 00:05:00.780
We'd like enough ways to avoid the cache line
conflicts we experienced with the DM cache.

00:05:00.780 --> 00:05:06.349
Looking at the graph we saw earlier of memory
accesses vs. time, we see that in any time

00:05:06.349 --> 00:05:12.110
interval there are only so many potential
address conflicts that we need to worry about.

00:05:12.110 --> 00:05:16.780
The mapping from addresses to cache lines
is designed to avoid conflicts between neighboring

00:05:16.780 --> 00:05:18.460
locations.

00:05:18.460 --> 00:05:25.150
So we only need to worry about conflicts between
the different regions: code, stack and data.

00:05:25.150 --> 00:05:29.949
In the examples shown here there are three
such regions, maybe 4 if you need two data

00:05:29.949 --> 00:05:33.470
regions to support copying from one data region
to another.

00:05:33.470 --> 00:05:37.729
If the time interval is particularly large,
we might need double that number to avoid

00:05:37.729 --> 00:05:43.460
conflicts between accesses early in the time
interval and accesses late in the time interval.

00:05:43.460 --> 00:05:48.969
The point is that a small number of ways should
be sufficient to avoid most cache line conflicts

00:05:48.969 --> 00:05:51.490
in the cache.

00:05:51.490 --> 00:05:56.380
As with block size, it's possible to have
too much of a good thing: there's an optimum

00:05:56.380 --> 00:06:01.069
number of ways that minimizes the AMAT.

00:06:01.069 --> 00:06:05.020
Beyond that point, the additional circuity
needed to combine the hit signals from a large

00:06:05.020 --> 00:06:10.819
number of ways will start have a significant
propagation delay of its own, adding directly

00:06:10.819 --> 00:06:15.009
to the cache hit time and the AMAT.

00:06:15.009 --> 00:06:18.939
More to the point, the chart on the left shows
that there's little additional impact on the

00:06:18.939 --> 00:06:22.270
miss ratio beyond 4 to 8 ways.

00:06:22.270 --> 00:06:27.530
For most programs, an 8-way set-associative
cache with a large number of sets will perform

00:06:27.530 --> 00:06:34.880
on a par with the much more-expensive FA cache
of equivalent capacity.

00:06:34.880 --> 00:06:40.180
There's one final issue to resolve with SA
and FA caches.

00:06:40.180 --> 00:06:44.550
When there's a cache miss, which cache line
should be chosen to hold the data that will

00:06:44.550 --> 00:06:47.840
be fetched from main memory?

00:06:47.840 --> 00:06:53.120
That's not an issue with DM caches, since
each data block can only be held in one particular

00:06:53.120 --> 00:06:56.510
cache line, determined by its address.

00:06:56.510 --> 00:07:02.340
But in N-way SA caches, there are N possible
cache lines to choose from, one in each of

00:07:02.340 --> 00:07:03.680
the ways.

00:07:03.680 --> 00:07:08.310
And in a FA cache, any of the cache lines
can be chosen.

00:07:08.310 --> 00:07:10.120
So, how to choose?

00:07:10.120 --> 00:07:15.240
Our goal is to choose to replace the contents
of the cache line which will minimize the

00:07:15.240 --> 00:07:18.930
impact on the hit ratio in the future.

00:07:18.930 --> 00:07:24.120
The optimal choice is to replace the block
that is accessed furthest in the future (or

00:07:24.120 --> 00:07:26.080
perhaps is never accessed again).

00:07:26.080 --> 00:07:29.759
But that requires knowing the futureâ€¦

00:07:29.759 --> 00:07:35.009
Here's an idea: let's predict future accesses
by looking a recent accesses and applying

00:07:35.009 --> 00:07:36.629
the principle of locality.

00:07:36.629 --> 00:07:41.919
d7.36 If a block has not been recently accessed,
it's less likely to be accessed in the near

00:07:41.919 --> 00:07:44.039
future.

00:07:44.039 --> 00:07:50.650
That suggests the least-recently-used replacement
strategy, usually referred to as LRU: replace

00:07:50.650 --> 00:07:54.090
the block that was accessed furthest in the
past.

00:07:54.090 --> 00:08:00.720
LRU works well in practice, but requires us
to keep a list ordered by last use for each

00:08:00.720 --> 00:08:06.729
set of cache lines, which would need to be
updated on each cache access.

00:08:06.729 --> 00:08:10.979
When we needed to choose which member of a
set to replace, we'd choose the last cache

00:08:10.979 --> 00:08:13.300
line on this list.

00:08:13.300 --> 00:08:21.330
For an 8-way SA cache there are 8! possible
orderings, so we'd need log2(8!) = 16 state

00:08:21.330 --> 00:08:24.689
bits to encode the current ordering.

00:08:24.689 --> 00:08:28.550
The logic to update these state bits on each
access isn't cheap.

00:08:28.550 --> 00:08:34.799
Basically you need a lookup table to map the
current 16-bit value to the next 16-bit value.

00:08:34.799 --> 00:08:39.610
So most caches implement an approximation
to LRU where the update function is much simpler

00:08:39.610 --> 00:08:42.539
to compute.

00:08:42.539 --> 00:08:47.660
There are other possible replacement policies:
First-in, first-out, where the oldest cache

00:08:47.660 --> 00:08:50.460
line is replaced regardless of when it was
last accessed.

00:08:50.460 --> 00:08:57.670
And Random, where some sort of pseudo-random
number generator is used to select the replacement.

00:08:57.670 --> 00:09:01.520
All replacement strategies except for random
can be defeated.

00:09:01.520 --> 00:09:05.330
If you know a cache's replacement strategy
you can design a program that will have an

00:09:05.330 --> 00:09:10.760
abysmal hit rate by accessing addresses you
know the cache just replaced.

00:09:10.760 --> 00:09:16.090
I'm not sure I care about how well a program
designed to get bad performance runs on my

00:09:16.090 --> 00:09:21.430
system, but the point is that most replacement
strategies will occasionally cause a particular

00:09:21.430 --> 00:09:25.949
program to execute much more slowly than expected.

00:09:25.949 --> 00:09:31.060
When all is said and done, an LRU replacement
strategy or a close approximation is a reasonable

00:09:31.060 --> 00:09:31.490
choice.